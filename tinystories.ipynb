{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:42:26.601853Z","iopub.execute_input":"2025-02-16T11:42:26.602188Z","iopub.status.idle":"2025-02-16T11:42:29.003662Z","shell.execute_reply.started":"2025-02-16T11:42:26.602167Z","shell.execute_reply":"2025-02-16T11:42:29.002636Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"dataset = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:42:29.005166Z","iopub.execute_input":"2025-02-16T11:42:29.005755Z","iopub.status.idle":"2025-02-16T11:42:53.636221Z","shell.execute_reply.started":"2025-02-16T11:42:29.005713Z","shell.execute_reply":"2025-02-16T11:42:53.635328Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c875d35035041a2901ecdfe50c96495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00004-2d5a1467fff1081b.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6836ba91614642a0996918e188b41ad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00001-of-00004-5852b56a2bd28fd9.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b37bc9bfc202410d92f23d5580c91a29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00002-of-00004-a26307300439e943.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f6dc0ebceba428aac4ed34e6fc3d135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00003-of-00004-d243063613e5a057.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1af066065964c1f90f94cbc042c5679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-869c898b519ad725.parquet:   0%|          | 0.00/9.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f93259fab95145698113c4fe70bd2af9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92fa8dba3c464a2e8432cb136e11ab90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4494e67d781146b2879065b0b57d1039"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"for i in dataset[\"train\"]:\n    print(i[\"text\"])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:42:53.637560Z","iopub.execute_input":"2025-02-16T11:42:53.638038Z","iopub.status.idle":"2025-02-16T11:42:53.643406Z","shell.execute_reply.started":"2025-02-16T11:42:53.638007Z","shell.execute_reply":"2025-02-16T11:42:53.642548Z"}},"outputs":[{"name":"stdout","text":"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n\nTogether, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import GPT2TokenizerFast\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:43:03.161448Z","iopub.execute_input":"2025-02-16T11:43:03.161821Z","iopub.status.idle":"2025-02-16T11:43:19.833323Z","shell.execute_reply.started":"2025-02-16T11:43:03.161794Z","shell.execute_reply":"2025-02-16T11:43:19.832356Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7689200c64746498ba68858488bbbe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b0013c400941a780a3556be1d0ae94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a37b6c73074ea688d45aba6f81d295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7be2ffa771484782c0d270c00cd8b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69703bd76444a60a6e665965834502e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cfc4a57d4724ac396fa3e09c73b64ab"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"vocab_size =tokenizer.vocab_size\nprint(f\"Vocabulary size: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:43:19.834659Z","iopub.execute_input":"2025-02-16T11:43:19.835266Z","iopub.status.idle":"2025-02-16T11:43:19.839952Z","shell.execute_reply.started":"2025-02-16T11:43:19.835233Z","shell.execute_reply":"2025-02-16T11:43:19.838994Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 50257\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\n# Function to create a causal mask\ndef generate_causal_mask(seq_len):\n    mask = torch.tril(torch.ones(seq_len, seq_len))  # Lower triangular matrix\n    return mask\n\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size, context_size, embedd_dim, num_heads):\n        super().__init__()\n        self.embedd_dim = embedd_dim  #\n        self.embedding_token = nn.Embedding(vocab_size, embedd_dim)\n        self.embedding_pos = nn.Embedding(context_size, embedd_dim)\n        self.trf_blocks = nn.ModuleList(\n            [TransformerBlock(num_heads=num_heads, emb_dim=embedd_dim) for _ in range(2)]\n        )\n        self.final_linear = nn.Linear(embedd_dim, vocab_size)  # Match embedding size\n\n    def forward(self, x, mask):\n        batch_size, seq_len = x.shape\n        tok_embeds = self.embedding_token(x)\n        pos_embeds = self.embedding_pos(torch.arange(seq_len, device=x.device))\n        x = tok_embeds + pos_embeds  \n        for block in self.trf_blocks:\n            x = block(x, mask)  \n        logits = self.final_linear(x)\n        return logits\n\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, num_heads, emb_dim):\n        super().__init__()\n        self.multi_head = MultiHeadAttention(num_heads, emb_dim)\n        self.norm_1 = LayerNorm(emb_dim=emb_dim)\n        self.norm_2 = LayerNorm(emb_dim=emb_dim)\n        self.feed_nn = FeedForwardNN(emb_dim)\n\n    def forward(self, x, mask):\n        skip_con1 = x\n        x = self.multi_head(x, mask)\n        x = self.norm_1(skip_con1 + x)\n\n        skip_con2 = x\n        x = self.feed_nn(x)  # Incorrect: was `self.FeedForwardNN(x)`\n        x = self.norm_2(skip_con2 + x)\n        return x\n        \n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, emb_dim):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [SelfAttention(emb_dim, emb_dim) for _ in range(num_heads)]\n        )\n        self.out_linear = nn.Linear(emb_dim * num_heads, emb_dim) \n\n    def forward(self, x, mask):\n        context_vec = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n        return self.out_linear(context_vec)\n        \n\nclass SelfAttention(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.query = nn.Linear(input_size, output_size)\n        self.key = nn.Linear(input_size, output_size)\n        self.value = nn.Linear(input_size, output_size)\n    \n    def forward(self, x, mask):\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (k.shape[-1] ** 0.5)  # Fixed transpose\n        \n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_probs = torch.softmax(attn_scores, dim=-1)  # Fixed double scaling\n        out = torch.matmul(attn_probs, v)\n        return out\n        \nclass FeedForwardNN(nn.Module):\n    def __init__(self, emb_dim, hidden_neurons=128):\n        super().__init__()\n        self.layer1 = nn.Linear(emb_dim, hidden_neurons)\n        self.layer2 = nn.Linear(hidden_neurons, emb_dim)  # Ensure output matches emb_dim\n        self.activation = GELU()\n        \n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.activation(x)\n        x = self.layer2(x)\n        return x\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:46:17.406739Z","iopub.execute_input":"2025-02-16T11:46:17.407100Z","iopub.status.idle":"2025-02-16T11:46:17.433381Z","shell.execute_reply.started":"2025-02-16T11:46:17.407077Z","shell.execute_reply":"2025-02-16T11:46:17.432314Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom transformers import GPT2TokenizerFast\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GPTStories(Dataset):\n    def __init__(self, texts, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        for txt in texts:  # Iterate through multiple documents\n            token_ids = tokenizer.encode(txt, max_length=1024, truncation=True)\n\n            for i in range(0, len(token_ids) - max_length, stride):\n                input_chunk = token_ids[i:i + max_length]\n                target_chunk = token_ids[i + 1: i + max_length + 1]\n                self.input_ids.append(torch.tensor(input_chunk))\n                self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n        \ndef create_dataloader(texts, batch_size=4, max_length=256, stride=128, shuffle=True):\n    \n    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n    \n    dataset = GPTStories(texts, tokenizer, max_length, stride)\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    return dataloader\n\n# ====== Causal Mask Generation ======\ndef generate_causal_mask(seq_len):\n    return torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)  # Shape (1, seq_len, seq_len)\n\n# ====== Training Loop ======\ndef train(model, dataloader, optimizer, criterion, num_epochs=5, device=\"cuda\"):\n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        model.train()\n\n        for batch_idx, (x, y) in enumerate(dataloader):\n            x, y = x.to(device), y.to(device)\n            mask = generate_causal_mask(x.shape[1]).to(device)\n\n            optimizer.zero_grad()\n            logits = model(x, mask)  # Forward pass\n            loss = criterion(logits.view(-1, logits.shape[-1]), y.view(-1))  # Cross-entropy loss\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        \n        avg_loss = total_loss / len(dataloader)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n# ====== Model Initialization ======\nvocab_size = 50257\nembedding_dim = 128 \nbatch_size = 128\nmax_length = 32\nnum_heads = 4\nsample_data = [i[\"text\"] + \"<|endoftext|>\" for i in dataset[\"validation\"]]\n\ndataloader = create_dataloader(\n    sample_data,\n    batch_size=batch_size,\n    max_length=max_length,\n    stride=max_length\n)\n\nmodel = GPT(vocab_size=vocab_size, context_size=max_length, embedd_dim=embedding_dim, num_heads=num_heads)\noptimizer = optim.Adam(model.parameters(), lr=3e-4)\ncriterion = nn.CrossEntropyLoss()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:48:23.682462Z","iopub.execute_input":"2025-02-16T11:48:23.682830Z","iopub.status.idle":"2025-02-16T11:48:43.387728Z","shell.execute_reply.started":"2025-02-16T11:48:23.682802Z","shell.execute_reply":"2025-02-16T11:48:43.386779Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:48:55.771951Z","iopub.execute_input":"2025-02-16T11:48:55.772285Z","iopub.status.idle":"2025-02-16T11:48:55.777812Z","shell.execute_reply.started":"2025-02-16T11:48:55.772258Z","shell.execute_reply":"2025-02-16T11:48:55.776849Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (embedding_token): Embedding(50257, 128)\n  (embedding_pos): Embedding(32, 128)\n  (trf_blocks): ModuleList(\n    (0-1): 2 x TransformerBlock(\n      (multi_head): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x SelfAttention(\n            (query): Linear(in_features=128, out_features=128, bias=True)\n            (key): Linear(in_features=128, out_features=128, bias=True)\n            (value): Linear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (out_linear): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (norm_1): LayerNorm()\n      (norm_2): LayerNorm()\n      (feed_nn): FeedForwardNN(\n        (layer1): Linear(in_features=128, out_features=128, bias=True)\n        (layer2): Linear(in_features=128, out_features=128, bias=True)\n        (activation): GELU()\n      )\n    )\n  )\n  (final_linear): Linear(in_features=128, out_features=50257, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"def print_model_params(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"Total Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n\n    # Print individual layer parameter sizes\n    # for name, param in model.named_parameters():\n    #     print(f\"{name}: {param.size()} -> {param.numel()} params\")\n\n# Example usage\nprint_model_params(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:48:57.931889Z","iopub.execute_input":"2025-02-16T11:48:57.932197Z","iopub.status.idle":"2025-02-16T11:48:57.937953Z","shell.execute_reply.started":"2025-02-16T11:48:57.932176Z","shell.execute_reply":"2025-02-16T11:48:57.937111Z"}},"outputs":[{"name":"stdout","text":"Total Parameters: 13,514,833\nTrainable Parameters: 13,514,833\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Train the model\ntrain(model, dataloader, optimizer, criterion, num_epochs=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:49:03.461349Z","iopub.execute_input":"2025-02-16T11:49:03.461719Z","iopub.status.idle":"2025-02-16T12:00:02.637993Z","shell.execute_reply.started":"2025-02-16T11:49:03.461689Z","shell.execute_reply":"2025-02-16T12:00:02.637121Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 4.7566\nEpoch 2/10, Loss: 3.5525\nEpoch 3/10, Loss: 3.2829\nEpoch 4/10, Loss: 3.1303\nEpoch 5/10, Loss: 3.0274\nEpoch 6/10, Loss: 2.9512\nEpoch 7/10, Loss: 2.8911\nEpoch 8/10, Loss: 2.8421\nEpoch 9/10, Loss: 2.8007\nEpoch 10/10, Loss: 2.7649\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"sample_data = [i[\"text\"] + \"<|endoftext|>\" for idx, i in enumerate(dataset[\"train\"]) if idx<100]\n\ntest_dataloader = create_dataloader(\n    sample_data,\n    batch_size=batch_size,\n    max_length=max_length,\n    stride=max_length\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:04:10.032727Z","iopub.execute_input":"2025-02-16T12:04:10.033022Z","iopub.status.idle":"2025-02-16T12:04:38.684865Z","shell.execute_reply.started":"2025-02-16T12:04:10.033001Z","shell.execute_reply":"2025-02-16T12:04:38.683872Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2TokenizerFast\n\ndef logits_to_words(input_tokens, logits, tokenizer, idx):\n    \"\"\"\n    Convert logits to words by selecting the highest probability tokens and appending to input.\n\n    Args:\n        input_tokens (torch.Tensor): Current input tokens of shape (1, seq_len)\n        logits (torch.Tensor): Model output logits of shape (1, seq_len, vocab_size)\n        tokenizer (GPT2TokenizerFast): Tokenizer to decode tokens\n        idx (int): Index of the current word in generation\n\n    Returns:\n        torch.Tensor: Updated input tokens with predicted tokens appended\n    \"\"\"\n    # Get the most probable token index\n    predicted_id = torch.argmax(logits[:, -1, :], dim=-1).item()  # Get scalar value\n\n    # Ensure token index is valid\n    if predicted_id >= tokenizer.vocab_size:\n        raise ValueError(f\"Invalid token index detected (out of vocab range)\")\n\n    # Decode tokens into words\n    predicted_word = tokenizer.decode(predicted_id, skip_special_tokens=True)\n    current_text = tokenizer.decode(input_tokens.squeeze(0), skip_special_tokens=True)\n\n    # Append predicted token to input\n    return torch.cat([input_tokens, torch.tensor([[predicted_id]])], dim=-1)\n\n# Setup\ndevice = 'cpu'\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nmodel.to(device)\n\ndef generate_text(model, tokenizer, input_tokens, num_words=5):\n    \"\"\"Generates next words for a single sentence (batch size = 1).\"\"\"\n    input_tokens = input_tokens.to(device)\n    prev_tokens = input_tokens\n\n    for i in range(num_words):\n        # Ensure input_tokens shape remains (1, seq_len)\n        input_tokens = input_tokens[:, -32:]  # Keep last 64 tokens if sequence grows\n        mask = generate_causal_mask(input_tokens.shape[1]).to(device)\n        logits = model(input_tokens, mask)  # Forward pass\n        input_tokens = logits_to_words(input_tokens, logits, tokenizer, i)\n\n    return tokenizer.decode(prev_tokens.squeeze(0), skip_special_tokens=True) +\" (pred started here) \"+ tokenizer.decode(input_tokens.squeeze(0)[-num_words:], skip_special_tokens=True)\n\n# Example usage (batch size = 1)\nfor batch_idx, (x, y) in enumerate(test_dataloader):\n    x = x[:1].to(device)  # Take only one sentence (batch size = 1)\n    output_text = generate_text(model, tokenizer, x, num_words=6)\n    print(\"\\nFinal Output:\\n\", output_text)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:21:55.901122Z","iopub.execute_input":"2025-02-16T12:21:55.901427Z","iopub.status.idle":"2025-02-16T12:21:56.398856Z","shell.execute_reply.started":"2025-02-16T12:21:55.901405Z","shell.execute_reply":"2025-02-16T12:21:56.397934Z"}},"outputs":[{"name":"stdout","text":"\nFinal Output:\n . Tim did not like being wet, so he went back inside his house. His mom saw his wet vest and told him to take it off.\n\n (pred started here) Timmy's mommy said\n","output_type":"stream"}],"execution_count":47}]}